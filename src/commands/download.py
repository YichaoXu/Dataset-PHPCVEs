"""
Download command module for PHP CVE Dataset Collection Tool.

This module provides functionality to download vulnerable code versions from
GitHub repositories based on the collected dataset.
"""

import time
from pathlib import Path
from typing import Optional, List
import typer
from rich.console import Console

from src.utils.logger import Logger
from src.utils.file_utils import ensure_dir, read_csv_file
from src.utils.github import GitHubAPI
from src.utils.ui import ProgressUI
from src.config import config

console = Console()

# Command-specific directories
DOWNLOAD_INTER_DIR = config.inter_dir / "download"
DEFAULT_OUTPUT_DIR = Path("downloads")

def download(
    input_file: Path = typer.Argument(..., help="Path to CSV file generated by collect command"),
    output_dir: Path = typer.Option(DEFAULT_OUTPUT_DIR, help="Directory to store downloaded code"),
    token: Optional[str] = typer.Option(None, help="GitHub API token"),
    cwe_filter: Optional[List[str]] = typer.Option(None, help="Filter by CWE IDs (e.g. 79,89)"),
    project_filter: Optional[List[str]] = typer.Option(None, help="Filter by project types"),
    limit: int = typer.Option(0, help="Limit number of records to process (0 for all)"),
    force: bool = typer.Option(False, help="Force download even if files exist"),
    verbose: bool = typer.Option(False, help="Enable verbose output")
):
    """
    Download vulnerable code versions from GitHub repositories.
    
    This command downloads vulnerable code versions from GitHub repositories based on
    the collected dataset, organizing them by CWE type and CVE ID.
    """
    # Enable verbose logging if requested
    Logger.set_verbose(verbose)
    
    # Start timing
    start_time = time.time()
    
    # Create output directory
    ensure_dir(output_dir)
    
    # Create intermediate directory
    ensure_dir(DOWNLOAD_INTER_DIR)
    
    # Initialize GitHub API client
    github_api = GitHubAPI(token=token)
    
    # Read input file
    Logger.info(f"Reading input file: {input_file}")
    records = read_csv_file(input_file)
    
    if not records:
        Logger.error("No records found in input file")
        raise typer.Exit(1)
    
    # Apply filters
    filtered_records = _apply_filters(records, cwe_filter, project_filter)
    
    # Apply limit
    if limit > 0 and limit < len(filtered_records):
        filtered_records = filtered_records[:limit]
        Logger.info(f"Limited to {limit} records")
    
    # Download code
    Logger.info(f"Downloading code for {len(filtered_records)} records")
    
    # Process each record
    with ProgressUI(len(filtered_records), "Downloading code") as progress:
        for record in filtered_records:
            cve_id = record.get("cve_id", "unknown")
            cwe_id = record.get("cwe_id", "unknown")
            repo_url = record.get("repository", "")
            previous_commit = record.get("previous_commit", "")
            
            progress.update(0, cve_id)
            
            # Skip if missing required information
            if not repo_url or not previous_commit:
                progress.log_warning(f"Skipped {cve_id}: Missing repository or previous commit information")
                progress.update(1, cve_id)
                continue
            
            # Create directories
            cwe_dir = output_dir / cwe_id
            cve_dir = cwe_dir / cve_id
            
            if not force and (cve_dir / "previous").exists():
                progress.log(f"Skipped {cve_id}: Already downloaded")
                progress.update(1, cve_id)
                continue
            
            ensure_dir(cwe_dir)
            ensure_dir(cve_dir)
            
            # Extract owner and repo from URL
            repo_path = repo_url.replace("https://github.com/", "")
            parts = repo_path.split('/')
            
            if len(parts) < 2:
                progress.log_error(f"Invalid repository URL format: {repo_url}")
                progress.update(1, cve_id)
                continue
                
            owner, repo = parts[0], parts[1]
            
            # Create directory for previous version
            ensure_dir(cve_dir / "previous")
            
            # Download previous version
            progress.update(0, f"{cve_id} (previous)")
            success = github_api.download_commit(
                owner, repo, previous_commit, cve_dir / "previous"
            )
            
            # Update progress
            if success:
                progress.log_success(f"Downloaded previous version for {cve_id}")
            else:
                progress.log_error(f"Failed to download code for {cve_id}")
            
            progress.update(1, cve_id)
    
    # Report timing
    elapsed_time = time.time() - start_time
    Logger.info(f"Download completed in {elapsed_time:.2f} seconds")
    
    return len(filtered_records)

def _apply_filters(records, cwe_filter, project_filter):
    """Apply filters to records."""
    filtered_records = records.copy()
    
    # Filter by CWE ID
    if cwe_filter:
        cwe_ids = [f"CWE-{cwe}" if not cwe.startswith("CWE-") else cwe for cwe in cwe_filter]
        filtered_records = [r for r in filtered_records if r.get("cwe_id") in cwe_ids]
        Logger.info(f"Filtered to {len(filtered_records)} records with CWE IDs: {', '.join(cwe_ids)}")
    
    # Filter by project type
    if project_filter:
        filtered_records = [r for r in filtered_records if r.get("project_type") in project_filter]
        Logger.info(f"Filtered to {len(filtered_records)} records with project types: {', '.join(project_filter)}")
    
    return filtered_records 